%YAML 1.1
---
# CPAC Pipeline Configuration YAML file
# Version 1.8.1
#
# http://fcp-indi.github.io for more info.
#
# Tip: This file can be edited manually with a text editor for quick modifications.

FROM: default


pipeline_setup: 

  # Name for this pipeline configuration - useful for identification.
  pipeline_name: cpac_ccs-options

  output_directory: 

    # Directory where C-PAC should write out processed data, logs, and crash reports.
    # - If running in a container (Singularity/Docker), you can simply set this to an arbitrary
    #   name like '/output', and then map (-B/-v) your desired output directory to that label.
    # - If running outside a container, this should be a full path to a directory.
    path: /outputs/output

    # Include extra versions and intermediate steps of functional preprocessing in the output directory.
    write_func_outputs: True

  working_directory: 

    # Directory where C-PAC should store temporary and intermediate files.
    # - This directory must be saved if you wish to re-run your pipeline from where you left off (if not completed).
    # - NOTE: As it stores all intermediate files, this directory can grow to become very
    #   large, especially for data with a large amount of TRs.
    # - If running in a container (Singularity/Docker), you can simply set this to an arbitrary
    #   name like '/work', and then map (-B/-v) your desired output directory to that label.
    # - If running outside a container, this should be a full path to a directory.
    # - This can be written to '/tmp' if you do not intend to save your working directory.
    path: /outputs/working

  log_directory: 

    path: /outputs/log

  system_config: 

    # The maximum amount of memory each participant's workflow can allocate.
    # Use this to place an upper bound of memory usage.
    # - Warning: 'Memory Per Participant' multiplied by 'Number of Participants to Run Simultaneously'
    #   must not be more than the total amount of RAM.
    # - Conversely, using too little RAM can impede the speed of a pipeline run.
    # - It is recommended that you set this to a value that when multiplied by
    #   'Number of Participants to Run Simultaneously' is as much RAM you can safely allocate.
    maximum_memory_per_participant: 10.0

# PREPROCESSING
# -------------
surface_analysis: 

  # Will run Freesurfer for surface-based analysis. Will output traditional Freesurfer derivatives.
  # If you wish to employ Freesurfer outputs for brain masking or tissue segmentation in the voxel-based pipeline,
  # select those 'Freesurfer-' labeled options further below in anatomical_preproc.
  run_freesurfer: On

  # Add extra arguments to recon-all command
  reconall_args: '-clean-bm -gcut'

anatomical_preproc: 

  # Non-local means filtering via ANTs DenoiseImage
  non_local_means_filtering:

    # this is a fork option
    run: [On]

  brain_extraction:

    # using: ['3dSkullStrip', 'BET', 'UNet', 'niworkflows-ants', 'FreeSurfer-ABCD', 'FreeSurfer-BET-Tight', 'FreeSurfer-BET-Loose']
    # this is a fork option
    using: ['FreeSurfer-BET-Tight', 'FreeSurfer-BET-Loose']

    FreeSurfer-BET:

      # Template to be used for FreeSurfer-BET brain extraction in CCS-options pipeline
      T1w_brain_template_mask_ccs: /ccs_template/MNI152_T1_1mm_first_brain_mask.nii.gz

segmentation:

  # Automatically segment anatomical images into white matter, gray matter,
  # and CSF based on prior probability maps.
  run: On

  tissue_segmentation:

    FreeSurfer:

      # Use mri_binarize --erode option to erode segmentation masks
      erode: 1

      # Label values corresponding to CSF in multiatlas file
      CSF_label : [4, 5, 43, 44, 31, 63]

      # Label values corresponding to Gray Matter in multiatlas file
      GM_label : [3, 42]

      # Label values corresponding to White Matter in multiatlas file
      WM_label : [2, 41, 7, 46, 251, 252, 253, 254, 255]


registration_workflows: 

  anatomical_registration: 

    # Register skull-on anatomical image to a template.
    reg_with_skull: Off

    registration: 

      # using: ['ANTS', 'FSL', 'FSL-linear']
      # this is a fork point
      #   selecting both ['ANTS', 'FSL'] will run both and fork the pipeline
      using: [FSL]

      FSL-FNIRT: 

        # Interpolation method for writing out transformed anatomical images.
        # Possible values: trilinear, sinc, spline
        interpolation: trilinear

  functional_registration: 

    coregistration:
      # functional (BOLD/EPI) registration to anatomical (structural/T1)

      func_input_prep:

        # Choose whether to use the mean of the functional/EPI as the input to functional-to-anatomical registration or one of the volumes from the functional 4D timeseries that you choose.
        # input: ['Mean Functional', 'Selected_Functional_Volume']
        input: ['Selected_Functional_Volume']

        Selected Functional Volume:

          # Only for when 'Use as Functional-to-Anatomical Registration Input' is set to 'Selected Functional Volume'.
          #Input the index of which volume from the functional 4D timeseries input file you wish to use as the input for functional-to-anatomical registration.
          func_reg_input_volume: 7

    func_registration_to_template: 

      FNIRT_pipelines:

        # Interpolation method for writing out transformed functional images.
        # Possible values: trilinear, sinc, spline
        interpolation: trilinear

functional_preproc: 

  despiking:

    # Run AFNI 3dDespike
    # this is a fork point
    #   run: [On, Off] - this will run both and fork the pipeline
    run: [On]

  slice_timing_correction:

    # Interpolate voxel time courses so they are sampled at the same time points.
    # this is a fork point
    #   run: [On, Off] - this will run both and fork the pipeline
    run: [On]

    # use specified slice time pattern rather than one in header
    tpattern: 'alt+z'

    # align each slice to given time offset
    # The default alignment time is the average of the 'tpattern' values (either from the dataset header or from the tpattern option).
    tzero: 0

  func_masking:

    # using: ['AFNI', 'FSL', 'FSL_AFNI', 'Anatomical_Refined', 'Anatomical_Based', 'CCS_Anatomical_Refined']
    # this is a fork point
    using: ['CCS_Anatomical_Refined']

nuisance_corrections: 

  2-nuisance_regression: 

    # this is a fork point
    #   run: [On, Off] - this will run both and fork the pipeline
    run: [Off]

    # switch to Off if nuisance regression is off and you don't want to write out the regressors
    create_regressors: Off

    # Select which nuisance signal corrections to apply
    Regressors: 
      - Name: Regressor-1
        Motion:
          include_delayed: On
          include_delayed_squared: On
          include_squared: On
        GlobalSignal:
          summary: Mean
        CerebrospinalFluid:
          erode_mask: Off
          extraction_resolution: 2
          summary: Mean
        WhiteMatter:
          erode_mask: Off
          extraction_resolution: 2
          summary: Mean
        PolyOrt:
          degree: 1
        Bandpass:
          bottom_frequency: 0.01
          top_frequency: 0.1
          method: AFNI
      - Name: Regressor-2
        Motion:
          include_delayed: On
          include_delayed_squared: On
          include_squared: On
        CerebrospinalFluid:
          erode_mask: Off
          extraction_resolution: 2
          summary: Mean
        WhiteMatter:
          erode_mask: Off
          extraction_resolution: 2
          summary: Mean
        PolyOrt:
          degree: 1
        Bandpass:
          bottom_frequency: 0.01
          top_frequency: 0.1
          method: AFNI

timeseries_extraction: 

  run: Off

seed_based_correlation_analysis: 

  # SCA - Seed-Based Correlation Analysis
  # For each extracted ROI Average time series, CPAC will generate a whole-brain correlation map.
  # It should be noted that for a given seed/ROI, SCA maps for ROI Average time series will be the same.
  run: Off

  # Enter paths to region-of-interest (ROI) NIFTI files (.nii or .nii.gz) to be used for seed-based correlation analysis, and then select which types of analyses to run.
  # Denote which analyses to run for each ROI path by listing the names below. For example, if you wish to run Avg and MultReg, you would enter: '/path/to/ROI.nii.gz': Avg, MultReg
  # available analyses:
  #   /path/to/atlas.nii.gz: Avg, DualReg, MultReg
  sca_roi_paths: 

    /cpac_templates/CC400.nii.gz: Avg

amplitude_low_frequency_fluctuation: 

  # ALFF & f/ALFF
  # Calculate Amplitude of Low Frequency Fluctuations (ALFF) and and fractional ALFF (f/ALFF) for all voxels.
  run: Off

regional_homogeneity: 

  # ReHo
  # Calculate Regional Homogeneity (ReHo) for all voxels.
  run: Off

voxel_mirrored_homotopic_connectivity: 

  # VMHC
  # Calculate Voxel-mirrored Homotopic Connectivity (VMHC) for all voxels.
  run: Off

network_centrality: 

  # Calculate Degree, Eigenvector Centrality, or Functional Connectivity Density.
  run: Off