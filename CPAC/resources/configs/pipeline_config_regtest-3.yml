%YAML 1.1
---
# CPAC Pipeline Configuration YAML file
# Version 1.8.0.dev
#
# http://fcp-indi.github.io for more info.
#
# Tip: This file can be edited manually with a text editor for quick modifications.

FROM: default


pipeline_setup: 

  # Name for this pipeline configuration - useful for identification.
  pipeline_name: regtest-3

  output_directory: 

    # Include extra versions and intermediate steps of functional preprocessing in the output directory.
    write_func_outputs: [On]

    # Include extra outputs in the output directory that may be of interest when more information is needed.
    write_debugging_outputs: [On]

    # Generate quality control pages containing preprocessing and derivative outputs.
    generate_quality_control_images: [On]

  log_directory: 

    path: /output

  system_config: 

    # The maximum amount of memory each participant's workflow can allocate.
    # Use this to place an upper bound of memory usage.
    # - Warning: 'Memory Per Participant' multiplied by 'Number of Participants to Run Simultaneously'
    #   must not be more than the total amount of RAM.
    # - Conversely, using too little RAM can impede the speed of a pipeline run.
    # - It is recommended that you set this to a value that when multiplied by
    #   'Number of Participants to Run Simultaneously' is as much RAM you can safely allocate.
    maximum_memory_per_participant: 3

    # The maximum amount of cores (on a single machine) or slots on a node (on a cluster/grid)
    # to allocate per participant.
    # - Setting this above 1 will parallelize each participant's workflow where possible.
    #   If you wish to dedicate multiple cores to ANTS-based anatomical registration (below),
    #   this value must be equal or higher than the amount of cores provided to ANTS.
    # - The maximum number of cores your run can possibly employ will be this setting multiplied
    #   by the number of participants set to run in parallel (the 'Number of Participants to Run
    #   Simultaneously' setting).
    max_cores_per_participant: 3

    # The number of cores to allocate to ANTS-based anatomical registration per participant.
    # - Multiple cores can greatly speed up this preprocessing step.
    # - This number cannot be greater than the number of cores per participant.
    num_ants_threads: 3

    # The number of participant workflows to run at the same time.
    # - The maximum number of cores your run can possibly employ will be this setting
    #   multiplied by the number of cores dedicated to each participant (the 'Maximum Number of Cores Per Participant' setting).
    num_participants_at_once: 15

  Amazon-AWS: 

    # Enable server-side 256-AES encryption on data to the S3 bucket
    s3_encryption: [Off]

# PREPROCESSING
# -------------

anatomical_preproc: 

  # Non-local means filtering via ANTs DenoiseImage
  non_local_means_filtering: On

  # N4 bias field correction via ANTs
  n4_bias_field_correction: On

  brain_extraction: 

    # Disables skull-stripping on the anatomical inputs if they are already skull-stripped outside of C-PAC.
    # Set this to True if your input images are already skull-stripped.
    already_skullstripped: [Off]

    extraction: 

      # using: ['3dSkullStrip', 'BET', 'UNet', 'niworkflows-ants']
      # this is a fork option
      using: [niworkflows-ants]

  segmentation_workflow: 

    # Automatically segment anatomical images into white matter, gray matter, and CSF based on prior probability maps.
    run: [On]

    1-segmentation: 

      Template_Based: 

        # These masks should be in the same space of your registration template, e.g. if
        # you choose 'EPI Template' , below tissue masks should also be EPI template tissue masks.
        #
        # Options: ['EPI Template', 'T1 Template']
        template_for_segmentation: [None]

    3-custom_thresholding: 

      # Use threshold to further refine the resulting segmentation tissue masks.
      run: [Customized Thresholding]

  registration_workflow: 

    # Register skull-on anatomical image to a template.
    reg_with_skull: [Off]

functional_preproc: 

  run: [On]

  truncation: 

    # First timepoint to include in analysis.
    # Default is 0 (beginning of timeseries).
    # First timepoint selection in the scan parameters in the data configuration file, if present, will over-ride this selection.
    # Note: the selection here applies to all scans of all participants.
    start_tr: 5

    # Last timepoint to include in analysis.
    # Default is None or End (end of timeseries).
    # Last timepoint selection in the scan parameters in the data configuration file, if present, will over-ride this selection.
    # Note: the selection here applies to all scans of all participants.
    stop_tr: 100

  despiking: 

    # Run AFNI 3dDespike
    # this is a fork point
    #   run: [On, Off] - this will run both and fork the pipeline
    run: [On]

  motion_estimates_and_correction: 

    # calculate motion statistics BEFORE slice-timing correction
    calculate_motion_first: [On]

    motion_correction: 

      # using: ['3dvolreg', 'mcflirt']
      # this is a fork point
      using: [mcflirt]

      # Choose motion correction reference. Options: mean, median, selected volume
      motion_correction_reference: [median]

  distortion_correction: 

    # option parameters
    PhaseDiff: 

      # Since the quality of the distortion heavily relies on the skull-stripping step, we provide a choice of method (AFNI 3dSkullStrip or FSL BET).
      fmap_skullstrip_option: [BET]

      # Set the fraction value for the skull-stripping of the magnitude file. Depending on the data, a tighter extraction may be necessary in order to prevent noisy voxels from interfering with preparing the field map.
      # The default value is 0.5.
      fmap_skullstrip_frac: [0.5]

  func_masking: 

    # using: ['AFNI', 'FSL', 'FSL_AFNI', 'Anatomical_Refined']
    # this is a fork point
    using: [FSL_AFNI]

nuisance_corrections: 

  1-ICA-AROMA: 

    # this is a fork point
    #   run: [On, Off] - this will run both and fork the pipeline
    run: [On, Off]

  2-nuisance_regression: 

    # Select which nuisance signal corrections to apply
    Regressors: 
      - Bandpass:
          bottom_frequency: 0.01
          top_frequency: 0.1
        CerebrospinalFluid:
          extraction_resolution: 2
          summary: Mean
        Motion:
          include_delayed: true
          include_delayed_squared: true
          include_squared: true
        PolyOrt:
          degree: 1
        aCompCor:
          extraction_resolution: 2
          summary:
            components: 5
            method: DetrendPC
          tissues:
          - WhiteMatter
          - CerebrospinalFluid
      - CerebrospinalFluid:
          erode_mask: true
          extraction_resolution: 2
          summary: Mean
        GlobalSignal:
          summary: Mean
        Motion:
          include_delayed: true
          include_delayed_squared: false
          include_squared: true
        aCompCor:
          extraction_resolution: 2
          summary:
            components: 5
            method: DetrendPC
          tissues:
          - CerebrospinalFluid
      - Censor:
          method: Kill
          number_of_previous_trs_to_censor: 1
          number_of_subsequent_trs_to_censor: 1
          thresholds:
          - type: FD_P
            value: 0.3
        Motion:
          include_delayed: true
          include_delayed_squared: false
          include_squared: false

    # Standard Lateral Ventricles Binary Mask
    # used in CSF mask refinement for CSF signal-related regressions
    lateral_ventricles_mask: /usr/share/fsl/5.0/data/atlases/HarvardOxford/HarvardOxford-lateral-ventricles-thr25-2mm.nii.gz

    # Whether to run frequency filtering before or after nuisance regression.
    # Options: 'After' or 'Before'
    bandpass_filtering_order: [Before]

functional_registration: 

  2-func_registration_to_template: 

    output_resolution: 

      # The resolution (in mm) to which the registered derivative outputs are written into.
      # NOTE:
      #   this is for the single-volume functional-space outputs (i.e. derivatives)
      #   thus, a higher resolution may not result in a large increase in RAM needs as above
      func_derivative_outputs: 2mm

    target_template: 

      # using: ['T1_template', 'EPI_template']
      # this is a fork point
      # NOTE:
      #   'EPI_template' registers the mean-functional directly to the template (chosen below under EPI_template)
      #   instead of applying the anatomical T1-to-template transform to the functional data that has been
      #   coregistered to anatomical/T1 space
      using: [T1_template, Off]

# OUTPUTS AND DERIVATIVES
# -----------------------
post_processing: 

  spatial_smoothing: 

    # Smooth the derivative outputs.
    run: [On, Off]

    # Tool to use for smoothing.
    # 'FSL' for FSL MultiImageMaths for FWHM provided
    # 'AFNI' for AFNI 3dBlurToFWHM for FWHM provided
    smoothing_method: [AFNI]

    # Choose whether to smooth outputs before or after z-scoring.
    # options: 'Before' or 'After'
    smoothing_order: [Before]

  z-scoring: 

    # z-score standardize the derivatives. This may be needed for group-level analysis.
    run: [On, Off]

timeseries_extraction: 

  run: [On]

  # Functional time-series and ROI realignment method: ['ROI_to_func'] or ['func_to_ROI']
  # 'ROI_to_func' will realign the atlas/ROI to functional space (fast)
  # 'func_to_ROI' will realign the functional time series to the atlas/ROI space
  #
  #     NOTE: in rare cases, realigning the ROI to the functional space may
  #           result in small misalignments for very small ROIs - please double
  #           check your data if you see issues
  realignment: [ROI_to_func]

  # By default, extracted time series are written as both a text file and a 1D file. Additional output formats are as a .csv spreadsheet or a Numpy array.
  roi_tse_outputs: [True, True]

seed_based_correlation_analysis: 

  # SCA - Seed-Based Correlation Analysis
  # For each extracted ROI Average time series, CPAC will generate a whole-brain correlation map.
  # It should be noted that for a given seed/ROI, SCA maps for ROI Average time series will be the same.
  run: [On]

amplitude_low_frequency_fluctuation: 

  # ALFF & f/ALFF
  # Calculate Amplitude of Low Frequency Fluctuations (ALFF) and and fractional ALFF (f/ALFF) for all voxels.
  run: [On]

regional_homogeneity: 

  # ReHo
  # Calculate Regional Homogeneity (ReHo) for all voxels.
  run: [On]

voxel_mirrored_homotopic_connectivity: 

  # VMHC
  # Calculate Voxel-mirrored Homotopic Connectivity (VMHC) for all voxels.
  run: [On]

network_centrality: 

  # Maximum amount of RAM (in GB) to be used when calculating Degree Centrality.
  # Calculating Eigenvector Centrality will require additional memory based on the size of the mask or number of ROI nodes.
  memory_allocation: 3.0

  # Full path to a NIFTI file describing the mask. Centrality will be calculated for all voxels within the mask.
  template_specification_file: s3://fcp-indi/resources/cpac/resources/mask-thr50-3mm.nii.gz

  degree_centrality: 

    # Enable/Disable degree centrality by selecting the connectivity weights
    #   weight_options: ['Binarized', 'Weighted']
    # disable this type of centrality with:
    #   weight_options: []
    weight_options: [True, True]

    # Select the type of threshold used when creating the degree centrality adjacency matrix.
    # options:
    #   'Significance threshold', 'Sparsity threshold', 'Correlation threshold'
    correlation_threshold_option: [Sparsity threshold]

  eigenvector_centrality: 

    # Enable/Disable eigenvector centrality by selecting the connectivity weights
    #   weight_options: ['Binarized', 'Weighted']
    # disable this type of centrality with:
    #   weight_options: []
    weight_options: [True, True]

    # Select the type of threshold used when creating the eigenvector centrality adjacency matrix.
    # options:
    #   'Significance threshold', 'Sparsity threshold', 'Correlation threshold'
    correlation_threshold_option: [Sparsity threshold]

  local_functional_connectivity_density: 

    # Enable/Disable lFCD by selecting the connectivity weights
    #   weight_options: ['Binarized', 'Weighted']
    # disable this type of centrality with:
    #   weight_options: []
    weight_options: [True, True]

    # Select the type of threshold used when creating the lFCD adjacency matrix.
    # options:
    #   'Significance threshold', 'Correlation threshold'
    correlation_threshold_option: [Significance threshold]

    # Based on the Threshold Type selected above, enter a Threshold Value.
    # P-value for Significance Threshold
    # Sparsity value for Sparsity Threshold
    # Pearson's r value for Correlation Threshold
    correlation_threshold: 0.001